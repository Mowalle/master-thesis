\chapter{Implementierung}
\label{chap:implementation}
% TODO: Dieses Kaptiel mit der Zeit anpassen

\section{Unity Engine}
\emph{Unity} \autocite{UnityTechnologies2018} ist eine \emph{Engine} zum Entwickeln von 2D- und 3D-Anwendung (vornehmlich Spiele).
Da Unity plattformunabhängig ist werden die Applikationen gleichzeitig für mehrere Plattformen entwickelt.
Erstellt werden die Anwendungen mit dem Unity Editor.

Eine Unity-Anwendung ist in Szenen unterteilt.
Jede Szene besitzt einen eigenen Szenengraph.
Dieser Graph verwaltet alle Objekte einer Szene.
Unity verwendet ein Entitäten-Komponenten-System.
Das heißt, alle Unity-Objekte sind im Kern attribut- und funktionslose Entitäten (in Unity \emph{GameObject}s genannt).
Die eigentliche Funktionalität wird erst durch Hinzufügen von Komponenten (\emph{Components}) zu den GameObjects deutlich.
Beispielsweise verfügt das GameObject eines Würfel-Objekts über die Komponenten \lstinline{Transform} (Position, Rotation und Skalierung), \lstinline{MeshRenderer} (Geometrie) und \lstinline{BoxCollider} (Kollision).
Neben Anwendungen für den Desktop oder mobile Endgeräte bietet Unity native Unterstützung für eine Vielzahl an AR- und VR-Plattformen an \parencite{UnityTechnologies2018b}.

Zusätzlich zu diesen vorgefertigten Komponenten können auch Skript-Komponenten zu den GameObjects hinzugefügt werden.
Dies ermöglicht Entwicklern, komplett neues Verhalten für Objekte in die Engine zu integrieren.
Die Skripte werden in C\# oder einem Javascript-Dialekt geschrieben.
Die Skripte erben von der Unity-Klasse \lstinline|MonoBehaviour|, wodurch sie in den Lebenszyklus von Unity-Objekten aufgenommen werden und Eventmethoden wie z.B. \lstinline|Start()|, \lstinline|Update()|, \lstinline|FixedUpdate()| usw. erhalten \parencite{UnityTechnologies2018c}.

Eine Besonderheit bei Unity im Vergleich zu anderen Engines sind die \emph{Prefabs}.
Dabei handelt es sich um abgespeicherte Konfigurationen von GameObjects inklusive derer Komponenten.
Die Prefabs können dann jederzeit als vorkonfiguriertes Objekt instanziiert werden.

Darüber hinaus lässt sich Unity durch das Importieren von Plugins erweitern.
So werden \emph{Assets} (Texturen, Fonts, Musik, Materialien, Skripte) und Prefabs von anderen Entwicklern in das aktuelle Projekt übernommen.

Aufgrund der einfach zu nutzenden AR-/VR-Funktionalität sowie der Erweiterbarkeit wird Unity für diese Arbeit eingesetzt.

\section{SteamVR Plugin}
Ein nennenswertes Plugin, was für diese Arbeit eingesetzt wird, ist das \emph{SteamVR Plugin} \autocite{ValveCorporation2018}.
Dieses baut die in Unity integrierte AR-/VR-Unterstützung weiter aus.

Eine zentrale Rolle nimmt das \emph{Player}-Prefab ein.
Durch Platzieren dieses Prefabs in der Unity-Szene werden automatisch GameObjects erzeugt, welche die Positionen und Rotationen vom HMD und weiteren Controllern eines verbundenen VR-Systems tracken.
Die Zuordnung der Geräte zu den GameObjects ist hardware-übergreifend und geschieht automatisch.
Zudem wird dem Nutzer eine Begrenzung des Spielebereichs angezeigt, welche sich aus der verwendeten Technologie ergibt (in diesem Fall die sationären \emph{Lighthouse} Basisstationen der Vive).
Ebenso werden 3D-Modelle der Hand angezeigt, wobei sich die Finger der virtuellen Hände zu den Knöpfen bewegen, die der Nutzer aktuell betätigt.
Das Prefab übernimmt außerdem das stereoskopische Rendern, sodass auf dem HMD für jedes Auge ein anderes Bild angezeigt wird, wodurch der Eindruck von räumlicher Tiefe entsteht.

Für diese Arbeit wird außerdem das \lstinline{Interactable}-SteamVR-Skript verwendet.
Durch dieses Skript können GameObjects mit einer \lstinline{Collider}-Komponente auf die virtuellen Hände bzw. Controller reagieren und interaktive gemacht werden (z.B. Aufnehmen und werfen von Objekten, anklicken von Knöpfen etc.).
Die Verwendungen dieses Skripts für den Megamap-Prototypen werden an entsprechender Stelle in den folgenden Abschnitten näher beschrieben.

\section{Virtuelle Laborumgebung}
Wie eingangs in \autoref{sec:motivation_ziel} erwähnt war die ursprüngliche Idee dieser Arbeit die Implementierung eines Megamap-Prototypen für das MR-HMD Magic Leap One.
Dabei sollte die virtuelle Karte mithilfe des HMDs in die reale Umgebung des Nutzers integriert werden.

Zu Beginn der Arbeit war allerdings statt der MR-Hardware nur ein Software-Simulator als Entwicklervorschau verfügbar.
Im Verlauf der Arbeit wurde die MR-Hardware veröffentlicht.
Das HMD war in der Arbeitsgruppe Human-Computer~ Interaction der Universität Bremen jedoch weiterhin nicht verfügbar.

Daher wurde in dieser Arbeit ein alternativer Prototyp für das VR-HMD HTC Vive entwickelt.
Wie die meisten VR-HMDs verdeckt die Vive das Sichtfeld des Nutzers komplett, um stattdessen die virtuellen Inhalte anzuzeigen.
Somit ist die reale Welt für die Nutzer nicht mehr sichtbar, was die visuelle Integration der Megamap in Umgebung unmöglich macht.

Der entwickelte Prototyp umgeht dieses Problem, indem ein virtuelles Modell der Umgebung im Maßstab 1:1 verwendet wird.
So wird die reale Welt um den Nutzer herum virtuell simuliert.
Die virtuelle Megamap kann dann in die \textit{virtuelle} Umgebung des Nutzers integriert werden.
Die Idee hinter diesem Ansatz ist, dass durch die Ähnlichkeit der realen und der virtuellen Umgebung das Prinzip des VR-Prototypen auf eine Situation mit einem MR-HMD weiterhin übertragbar ist.
In zukünftigen Arbeiten könnte damit der Megamap-Prototyp auf MR-HMDs eingesetzt werden, ohne grundlegende Änderungen vornehmen zu müssen.

Durch die Verwendung eines 3D-Modells wird außerdem die 3D-Rekonstruktionsfunktion der Magic Leap One nachgeahmt.
Wie auch die HoloLens erstellt die Magic Leap One durch aktives Tracking der Umwelt intern ein 3D-Modell der Umgebung.
Dieses wird für die Kollisionsberechnung mit den virtuellen Inhalten verwendet.
So können virtuelle Objekte mit realen Gegenständen interagieren (z.B. läuft ein Charakter über einen realen Tisch oder ein virtueller Bildschirm wird an einer realen Wand platziert).
Durch die Verwendung eines 3D-Modells der Umgebung im Megamap-Prototypen ist Möglichkeit der Interaktion mit der Umwelt implizit gegeben.

Für den Prototypen wurde ein maßstabsgetreues 3D-Modell des Laborraums der Arbeitsgruppe Human-Computer~ Interaction der Universität Bremen angefertigt, welcher der Ort der durchgeführten Nutzerstudie ist (siehe \autoref{chap:evaluation}).
\autoref{fig:lab_environment} zeigt ein Foto des Laborraums sowie Screenshots der entsprechenden virtuellen Umgebung.
Der Grundriss des Raums wurde im Modellierungsprogramm \emph{Blender} mithilfe des eingebauten \emph{Archimesh}-Werkzeugs zentimetergenau erstellt.
Das Modell wurde als \texttt{.fbx} in Unity importiert und mit diversen virtuellen Requisiten ergänzt, um den Raum realistischer und dem Original ähnlicher wirken zu lassen.

\begin{figure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.755\textwidth]{figures/photo_lab}
        \caption{}
        \label{sfig:lab_photo}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/lab3}
        \caption{}
        \label{sfig:lab_screenshot_1}
    \end{subfigure}

    \vspace{1em}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/lab1}
        \caption{}
        \label{sfig:lab_screenshot_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/lab2}
        \caption{}
        \label{sfig:lab_screenshot_3}
    \end{subfigure}
    \caption{Um die Situation der MR-Anwendung in VR nachzubilden, wurde der Laborraum im Maßstab 1:1 modelliert. %
             \subref{sfig:lab_photo} Foto des realen Laborraums. %
             \subref{sfig:lab_screenshot_1}--\subref{sfig:lab_screenshot_3} Screenshots des virtuellen Laborraums in Unity.%
	}
	\label{fig:lab_environment}
\end{figure}

\subsection*{Synchronisation der realen und virtuellen Position}
Damit die virtuelle Umgebung als Ersatz für die reale Umgebung verwendet werden kann muss sichergestellt werden, dass die Rotation des virtuellen Raums der des realen Raums entspricht \emph{und} dass die Position des Nutzers in der virtuellen Welt seiner Position in der realen Welt entspricht.
Der Prototyp muss dafür vorab in zwei Schritten konfiguriert werden:

\paragraph{Erster Schritt:}
Während der Kalibrierung der Vive legt der Nutzer den Spielebereich (die sogenannte \emph{Play Area}) fest.
Dies ist der Bereich, in dem sich keine Hindernisse für den Nutzer befinden und der für die Basisstationen sichtbar ist.
Hierfür zieht der Nutzer mit den Controllern ein Rechteck nach, welches dann als Spielebereich gesetzt wird.

Es wichtig, dass die längere Seite des Rechtecks orthogonal zur Ausgangsrotation des \lstinline{Player}-Objekts in Unity verläuft.
Die aktuell getrackte Rotation des HMDs wird relativ zur Ausrichtung der Play Area gemessen.
Da SteamVR automatisch die längeren Seiten der Play Area als Vorder- bzw. Rückseite betrachtet, müssen diese orthogonal zur Blickrichtung des \lstinline{Player}-Objekts verlaufen.
Im Fall des entwickelten Prototypen bedeutet dies, dass die Vorderseite der Play Area parallel zu der Wand des Labors verlaufen muss, an der sich die Eingangstür und Tische befinden (siehe \autoref{fig:lab_environment}).
Die korrekte Kalibrierung der Play Area wird in \autoref{fig:ve_setup_correct} skizziert.

Wenn bei der Kalibrierung der Play Area eine andere Ausrichtung gewählt wird, als die Ausgangsrotation des \lstinline{Player}-Objekts, weicht die virtuelle Rotation des Nutzers von der realen Rotation im Bezug zum Raum ab.
Effektiv hätte die virtuelle Umgebung dann eine andere Ausrichtung als die reale Umgebung.
Die falsche Kalibrierung wird in \autoref{fig:ve_setup_wrong} dargestellt.

Ginge es nur um den visuellen Reiz in der virtuellen Umgebung, würde der Nutzer diese Abweichung lediglich beim Aufsetzen des HMDs bemerken (da dann der virtuelle Raum anders rotiert wäre als der reale).
Wenn aber zum Beispiel der Tastsinn eine Rolle spielt, dann ist eine korrekte Ausrichtung des virtuellen Raums sinnvoll.
Da in dem entwickelten Prototyp nicht ausgeschlossen ist, dass der Nutzer die Play Area verlässt und beispielsweise die Säule oder Wände mit den Händen berührt, ist es für die Immersion von Vorteil, wenn das Berühren der virtuellen Objekte mit den taktilen Reizen der realen Objekte verknüpft wird.
Weiterhin wird durch die korrekte Ausrichtung die Messung des Abweichungsfehlers beim Zeigen auf Objekte erleichtert, was detailliert in \autoref{chap:evaluation} beschrieben wird.

\begin{figure}
    \includegraphics[width=\textwidth]{figures/environment_setup_correct}
    \caption{Die Play Area hat die gleiche Ausrichtung wie das \lstinline{Player}-Objekt in der Grundrotation (entlang z-Achse). %
    Die Rotation des Nutzers in der realen und virtuellen Welt stimmt überein.}
    \label{fig:ve_setup_correct}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{figures/environment_setup_wrong}
    \caption{Die Play Area hat \emph{nicht} die gleiche Ausrichtung wie das \lstinline{Player}-Objekt in der Grundrotation. %
        Der Nutzer hat einen Rotationsoffset (hier \ang[detect-weight=true]{90}), wodurch für den Nutzer effektiv die Räume unterschiedlich rotiert wirken.}
    \label{fig:ve_setup_wrong}
\end{figure}

\paragraph{Zweiter Schritt:}
Neben der Orientierung des Nutzers muss die Position des virtuellen Raums angepasst werden, wenn dieser die reale Umgebung bestmöglich überlagern soll.
Dies resultiert (wie bei der Rotation) aus der Tatsache, dass die getrackte Position des HMDs im Bezug zur Play Area gemessen wird, welche sich durch den Kalibrierungsvorgang von SteamVR ändern kann.
Um neue Position des virtuellen Raums zu bestimmen wird für den Prototypen wie folgt vorgegangen:

Der Prototyp wird mit seiner Ausgangsposition im Unity Editor ausgeführt.
Die Ecke \enquote{links unten} aus Vogelperspektive des virtuellen Labors befindet sich an der Unity-Welt-Koordinate $(0, 0, 0)$.
Die Controller, welche im HMD durch 3D-Modelle visualisiert sind, werden in den Ecken des virtuellen Raums platziert.
Dabei wird darauf geachtet, dass sie von den Basisstationen weiterhin erkannt werden.
Nun wird im realen Labor die Entfernung von den Controllern zu den entsprechenden Ecken des Raums gemessen, was der Abweichung der virtuellen Welt entspricht.
Die Entfernungen werden gemittelt.
Der resultierende Offset wird dann im Unity Editor verwendet, um das virtuelle Labor zu verschieben.
Dieser Offset ist nun solange gültig, bis die Play Area neu kalibriert wird.

Wie auch schon bei der Rotation ist diese Angleichung erst dann für die Nutzer bemerkbar, wenn sie versuchen physische Objekte wie Wände oder die Säule zu berühren.
Würde die Verschiebung des Raums nicht durchgeführt werden, könnten Nutzer mit den Controllern durch die virtuellen Objekte hindurch greifen oder sie würden auf physische Barrieren stoßen, obwohl die virtuellen Objekte noch weiter entfernt sind.

Nach diesen beiden Schritten kann der virtuelle Raum als Ersatz für die reale Umgebung verwendet werden, die in einer MR-Anwendung ohne vorheriges Modellieren verfügbar wäre.

\section{Das Megamap-GameObject}
\begin{figure}[t]
    \centering
    \imagebox{\includegraphics[height=8.5cm]{figures/unity_megamap_object}}
    \caption{Übersicht des Megamap-GameObjects.}
    \label{fig:unity_megamap_object}
\end{figure}

Eine grundlegende Implementierung des in \autoref{chap:concept} vorgestellten Megamap-Konzepts wird im Megamap-GameObject realisiert.
Eine strukturelle Übersicht des Objekts wird in \autoref{fig:unity_megamap_object} gegeben.
Das Objekt setzt sich aus drei Teilen zusammen, die jeweils durch eigene Komponenten repräsentiert werden:

Das Skript \textbf{\lstinline{Megamap}} bietet allgemeine Einstellungsmöglichkeiten zur Darstellung der Megamap.
Es lassen sich z.B. die Skalierung oder die Höhe vom Boden ändern.
Weiterhin bietet das Skript die Methoden \lstinline{Show()} und \lstinline{Hide()}, mit denen die Karte angezeigt bzw. ausgeblendet werden kann.
Falls dabei das Feld \lstinline{useAnimation} auf \lstinline{true} gesetzt ist, wird die Karte beim Anzeigen von einer 1:1 Raumgröße auf die eingestellte Skalierung herunterskaliert (beim Ausblenden analog ein Hochskalieren).
Die Megamap wird standardmäßig Nutzer-zentriert platziert.
Das bedeutet, dass der Nutzer sowohl in der realen Welt als auch auf der Karte die gleiche Position einnimmt.
Hierfür wird die virtuelle Umgebung referenziert und der Abstand vom HMD zum Ursprung des Labor-Modells ermittelt.
Dieser Abstand wird dann wie die Megamap skaliert und die Karte wird um den skalierten Abstand verschoben.
Sowohl die Animation als auch die Zentrierung auf den Nutzer sollen es ihm erleichtern, seine eigene Position auf der Karte zu finden.
Außerdem soll hierdurch der Bezug vom Laborraum auf der Karte zum umgebenden Laborraum hervorgehoben werden.

Das \lstinline|Megamap|-Skript dient lediglich als Schnittstelle zur Manipulation der Karte.
Das eigentliche 3D-Modell, welches als Karte verwendet wird, ist von diesem Skript unabhängig und wird erst zur Laufzeit durch andere Skripte als aktive Karte gesetzt.
Hierfür bietet das \lstinline|Megamap|-Skript die Methode \lstinline|SetMap(...)| an, welche als Parameter unter anderem eine Referenz auf ein \lstinline|IndoorMap|-Skript erwartet.
Dank dieser Trennung von Schnittstelle und Modell können zur Laufzeit unterschiedliche Indoor-Karten gewechselt werden, was für die Nutzerstudie in \autoref{chap:evaluation} Voraussetzung ist.
Die Details zum \lstinline|IndoorMap|-Skript werden in \autoref{sec:indoor_maps} ausgeführt.

\begin{figure}
    \centering
    \begin{minipage}[t]{.485\textwidth}
        \centering
        \vspace{0pt}
        \includegraphics[width=\linewidth]{figures/megamap_user_marker.pdf}
        \captionof{figure}{Der User Marker zeigt die Position des Nutzers auf der Megamap an. %
            \textcolor{red}{A:} Marker in Umgebung. %
            \textcolor{red}{B:} Marker auf Karte.%
        }
        \label{fig:user_marker}
        \vfill
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.485\textwidth}
        \centering
        \vspace{0pt}
        \includegraphics[width=\linewidth]{figures/megamap_room_guides}
        \captionof{figure}{Die Room Guides verbinden die Ecken des umgebenden Raums mit dem entsprechenden Raum auf der Megamap.}
        \label{fig:room_guides}
    \end{minipage}
\end{figure}

Der zweite Teil des Megamap-Objekts, das \textbf{\lstinline|User Marker|} Skript, dient ebenfalls der Orientierung für den Nutzer.
Analog zu Kartenanwendungen wie Google Maps befindet sich ein Marker auf der Megamap, welcher die aktuelle Position des Nutzers anzeigt.
Da es sich bei der Megamap um eine dreidimensionale Karte handelt wird hier ein Zylinder in Form ähnlich eines Pucks benutzt (siehe \autoref{fig:user_marker}).
Mittels eines transparenten Kreissegments wird außerdem die aktuelle Rotation des Nutzers verdeutlicht.
Um den Bezug vom Marker auf der Karte zur Umgebung zu verstärken ist auch in dieser ein Kreis platziert, welcher auf dem Nutzer zentriert ist.

Der dritte Teil des Megamap-Objekts ist das \textbf{\lstinline|Room Guides|} Skript.
Durch dieses werden Linien von den Ecken des umgebenden Laborraums zum Laborraum auf der Karte gezogen (siehe \autoref{fig:room_guides}).
Die Linien heben den Raum hervor, in dem sich der Nutzer befindet und stellen somit einen Bezug zur Umgebung her.
Implementiert sind diese Führungslinien als acht \lstinline|GameObject|s mit jeweils einer \lstinline|LineRenderer|-Komponente, die zusammen mit der Megamap ein- bzw. ausgeblendet werden.
Für die Startpunkte der Linien (die Ecken des umgebenden Raums) wird über die \lstinline|Renderer|-Komponente des Labor-Modells auf dessen \emph{Bounding Box} zugegriffen.
Die Eckpunkte der Bounding Box sind die Startpunkte der Linien.
Die Endpunkte der Linien werden berechnet, indem die Startpunkte mit der Transformationsmatrix der Megamap multipliziert werden.
So entsteht der Effekt, dass die Linien zu den Eckpunkten des Labors auf der Karte hinführen.
Standardmäßig werden über das Feld \lstinline|showUpperOnly| nur die oberen vier Linien angezeigt, da die unteren Führungslinien vom Rest der Megamap verdeckt werden würden.

\section{Erstellung der Indoor-Karten}
\label{sec:indoor_maps}
Wie bereits erwähnt ist das eigentliche 3D-Modell, welches das Layout des Gebäudes repräsentiert, vom \lstinline|Megamap|-Skript getrennt.
Für den in dieser Arbeit entwickelten Prototypen werden vorab erstellte 3D-Modelle eingesetzt, die in Blender mit dem Archimesh-Werkzeug modelliert wurden.
Ansätze zur automatisierten Generierung von Gebäudemodellen werden in \autoref{sec:building_data_automation} untersucht.

Die erstellten Karten sind 3D-Modelle, bei denen die einzelnen Räume als separate GameObjects in Unity verfügbar sind.
\autoref{fig:room_hierarchy} zeigt schematisch, wie die Objekthierarchie für die Indoor-Karten in Unity aufgebaut ist.

Für die Nutzerstudie aus \autoref{chap:evaluation} ist es notwendig, dass Nutzer mit Räumen interagieren können.
Es wird eine Suchaufgabe implementiert, bei der die Nutzer den Raum finden müssen, der die meisten Bälle enthält.
Die Nutzer können mit den Vive-Controllern Räume anvisieren und durch Betätigung des Triggers auswählen.
Handelt es sich um den gesuchten Raum, wird die Karte ausgeblendet.
Ansonsten wird der Raum als \enquote{falscher Raum} hervorgehoben.

\begin{figure}[tbh]
    \begin{minipage}[t]{0.35\textwidth}
        \centering
        \vspace{0pt}
        \imagebox{\includegraphics[height=9.7cm]{figures/unity_indoor_map}}
        \caption{Objekthierarchie des IndoorMap-Objekts,}
        \label{fig:room_hierarchy}    
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.64\textwidth}
        \centering
        \vspace{0pt}
        \begin{subfigure}{\linewidth}
            \missingfigure[figwidth=\linewidth]{Screenshot vom anvisierten Raum.}
            \caption{}
        \end{subfigure}
    
        \vspace{1em}
        \begin{subfigure}{\linewidth}
            \missingfigure[figwidth=\linewidth]{Screenshot von falsch geklicktem Raum.}
            \caption{}
        \end{subfigure}
    \caption{Screenshots.}
    \end{minipage}
\end{figure}

Diese Interaktionsmöglichkeit wird durch das \lstinline|SelectRoom|-Skript umgesetzt.
Das Skript verwendet die SteamVR-Komponente \lstinline|Interactable|, wodurch Räume auf An\-nä\-he\-rungs- und Interaktionsevents der Vive-Controller reagieren können.
Im Skript werden dazu spezielle SteamVR-Methoden implementiert.
Die Methode \lstinline|OnHandHoverBegin(...)| wird ausgelöst, sobald die \lstinline|HoverTransform| des Vive-Controllers den Collider des Raums berührt.
In der Methode wird der Raum gelb hervorgehoben, wenn er zuvor nicht angeklickt wurde.
Wenn er zuvor bereits angeklickt wurde (und nicht der gesuchte Raum ist) wird der Raum stattdessen rot eingefärbt.

Die Methode \lstinline|OnHandHoverEnd(...)| setzt die Farbe des Raums in den Normalzustand zurück, wenn der Vive-Controller den Collider des Raums verlässt.
Somit ist die farbliche Änderung nur sichtbar, solange der Raum mit dem Controller anvisiert wird.

Die \lstinline|HandHoverUpdate(...)|-Methode wird in jedem Update-Iteration von Unity ausgeführt, solange der Vive-Controller den Collider des Raums überlappt.
In der Methode wird überprüft, ob aktuell der Trigger des Controllers betätigt wird.
Sobald der Trigger vom Nutzer gedrückt wird, wird ein Event ausgelöst, was die Auswahl eines Raums signalisiert.
Handelt es sich dabei um den gesuchten Zielraum, wird das Event \lstinline|OnTargetRoomSelected| ausgelöst.
Dieses wird an anderer Stelle benutzt, um die Karte bei Auswahl des Zielraums auszublenden.
Wenn der falsche Raum gewählt wird, wird stattdessen das Event \lstinline|OnWrongRoomSelected| aktiviert.
Dieses wird genutzt, um die Auswahl von falschen Räumen zu zählen und in \autoref{chap:evaluation} auszuwerten. 

Um den Zugriff auf die Räume zu vereinfachen verfügt das IndoorMap-Objekt über das gleichnamige Skript \lstinline|IndoorMap|.
Dieses bietet C\#-Eigenschaften (\emph{Properties}) an, um die Liste aller Räume bzw. die Liste aller \emph{auswählbarer} Räume zu erhalten (\lstinline|Rooms| bzw. \lstinline|SelectableRooms|).

Damit die Megamap das IndoorMap-Objekt als Karte verwendet wird es an die Methode \lstinline|SetMap(...)| des \lstinline|Megamap|-Skripts übergeben.
Hierdurch wird das IndoorMap-Objekt zu einem Kind-Objekt der Megamap.
Die Skalierung und Translation der Megamap werden automatisch für das Kartenmodell übernommen.
Die Karte kann durch weitere Aufrufe von \lstinline|SetMap(...)| ausgetauscht werden.

\section{Interaktion mittels virtuellem Laserpointer}
Mit den \lstinline|SelectRoom|- und \lstinline|Interactable|-Skripten können Nutzer die Räume auf der Karte mit dem Vive-Controller auswählen.
Allerdings müssten sie sich (ohne weitere Maßnahmen) zu den einzelnen Räumen hinbewegen, um diese mit dem Vive-Controllern zu berühren und auszuwählen.
In informellen Vortests des Prototypen zeigte sich, dass die Bewegung zu den Räumen umständlich ist, wenn lediglich ein einzelner Raum gefunden werden soll.
Die Play Area der Vive schränkt die Bewegung der Nutzer zusätzlich ein, da außerhalb der Play Area mit Hindernissen gerechnet werden muss und das Tracking durch die Basisstationen ungenauer wird.

Für den aktuellsten Prototyp wurde daher eine Methode zur entfernten Auswahl von Räumen entwickelt.
Über einen virtuellen \enquote{Laser Pointer} können Nutzer mit den Vive-Controllern auf Räume zeigen und diese mit dem Trigger auswählen.
Das Skript \lstinline|LaserPointer| setzt dies um, indem es einen \lstinline|LineRenderer| aktiviert, welcher einen Lasterstrahl darstellt.
Der Strahl verläuft vom Controller geradeaus in die Szene.
In der Methode \lstinline|Update()| wird dann durch \lstinline|Physics.RaycastAll(...)| ein Schnitttest zwischen dem Strahl und Collidern in der Szene durchgeführt.
Falls ein Collider vom Strahl getroffen wird und der Collider zu einem der Räume gehört, wird der Schnittpunkt als Endpunkt des Laserstrahls gesetzt.
So entsteht der Effekt, dass der Laserstrahl nicht durch die Räume hindurchgeht.

Wie im vorigen Abschnitt beschrieben findet die Interaktion zwischen Vive-Controller und einem Raum statt, wenn der Controller den Raum überlappt.
Da jedoch beim Laserpointer der Controller vom Raum entfernt ist, muss hier eine Modifikation vorgenommen werden.
Intern nutzt das SteamVR-\lstinline|Hand|-Skript (welches den Controller repräsentiert und trackt) die Felder \lstinline|HoverSphereTransform| und \lstinline|HoverSphereRadius|.
Beide Felder definieren eine unsichtbare Kugel mit einem gegebenen Radius.
Standardmäßig folgt diese Kugel der Position des Controllers.
Sobald die Kugel mit einem \lstinline|Interactable|-Objekt überlappt, werden die entsprechenden SteamVR-Events ausgelöst.
Für den Laserpointer wird diese Kugel verschoben, wenn der Raycast einen Schnittpunkt mit einem auswählbaren Raum ergibt.
Die Position der Kugel wird auf den Schnittpunkt geändert, wodurch sie sich effektiv am Ende des Laserstrahls befindet.
Damit werden die SteamVR-Events für den entsprechenden Raum am Ende des Strahls ausgelöst.
\autoref{fig:laserpointer_sketch} verdeutlicht diesen Vorgang.
Wenn der Raycast keinen Schnittpunkt mit einem auswählbaren Raum ergibt, wird die Kugel wieder auf die Controller-Position zurückgesetzt.

\begin{figure}[tbh]
    \centering
    \imagebox{%
    \begin{subfigure}{0.45\linewidth}
        \includegraphics[width=\linewidth]{figures/laserpointer_sketch_no_hit}
        \caption{}
        \label{sfig:laserpointer_sketch_no_hit}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \includegraphics[width=\linewidth]{figures/laserpointer_sketch_hit}
        \caption{}
        \label{sfig:laserpointer_sketch_hit}
    \end{subfigure}
    }
    \caption{Funktionsweise des Laserpointers. %
        Die \lstinline|HoverSphere| wird verschoben, sodass der Raum auf den Vive-Controller reagiert. %
        \textbf{A:} Laserpointer. \textbf{B:} SteamVR-\lstinline|HoverSphere|. \textbf{C:} Auswählbarer Raum.}
    \label{fig:laserpointer_sketch}
\end{figure}

\begin{figure}[tbh]
\centering
\begin{subfigure}{0.49\linewidth}
    \missingfigure[figwidth=\linewidth]{LP Screenshot No Hit}
    \caption{}
    \label{sfig:laserpointer_screenshot_no_hit}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\linewidth}
    \missingfigure[figwidth=\linewidth]{LP Screenshot Hit}
    \caption{}
    \label{sfig:laserpointer_screenshot_hit}
\end{subfigure}
\end{figure}

\section{Automatisierung durch georeferenzierte Gebäudedaten}
\label{sec:building_data_automation}
Der beschriebene Ansatz hat den Nachteil, dass die Gabäudekarten vorab als 3D-Modell per Hand erstellt werden müssen.
Dies ist zum einen ein erheblicher zeitlicher Aufwand.
Zum anderen limitiert es die Anwendung auf die modellierten Gebäude.
Für eine (mobile) MR-Anwendung wäre es wünschenswert, wenn die Daten des Gebäudes, in dem sich der Nutzer aktuell befindet, für die Megamap verfügbar wären.

Im Rahmen dieser Arbeit wurden verschiedene Ansätze untersucht, georeferenzierte Gebäudedaten für die Megamap zu nutzen.
Bei der Implementierung der Ansätze offenbarten sich allerdings Probleme, welche die Integration von den georeferenzierten Daten in den aktuellen Prototypen unmöglich machten.
Im Sinne der Dokumentation und als Grundlage für zukünftige Arbeiten werden die untersuchten Ansätze dennoch in den folgenden Abschnitten beschrieben.
Leser, die lediglich am aktuellen Prototypen der Megamap interessiert sind, können mit \autoref{chap:evaluation} fortfahren.

\subsection{WRLD SDK}

Das \emph{WRLD SDK} (fortan \enquote{WRLD}) ein Framework, um interaktive 3D-Karten zu erstellen und anzuzeigen.
Es unterstützt mehrere Plattformen und bietet zudem ein Plugin für Unity an.
WRLD bezieht dabei seine Kartendaten von \emph{OpenSteetMap} sowie diversen proprietären Anbietern \parencite{WRLD2018}.
Die Daten werden aufbereitet und Nutzern über WRLDs eigene Server zur Verfügung gestellt.
Eine AR-Beispielanwendung ist in \autoref{fig:wrld_ar} zu sehen.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/wrld_ar-web-11}
    \caption{AR-Anwendung von WRLD, welche eine 3D-Ansicht von London auf einem Tisch platziert. \quelle{\cite{WRLD2018b}}}
    \label{fig:wrld_ar}
\end{figure}

Das Besondere an WRLD ist, dass es eine Funktion zur Einbettung von Indoor-Karten von Gebäuden bietet.
Hierzu können Entwickler georeferenzierte Lagepläne auf die WRLD-Server hochladen und für das gewünschte Gebäude registrieren.
Danach sind die Indoor-Karten für die Gebäude öffentlich für alle Nutzer von WRLD zugänglich.
Zudem können die Indoor-Karten mit Einrichtungsgegenständen versehen werden, um die Karten natürlicher aussehen zu lassen.
\autoref{fig:wrld_indoor} zeigt die Interaktion mit WRLDs Indoor-Karten.
In der Karten-3D-Ansicht kann über einen Button das Gebäude betreten werden (falls eine Indoor-Karte für das Gebäude vorhanden ist).
Über einen Slider kann das anzuzeigende Stockwerk ausgewählt werden.
Dabei werden alle Stockwerke des Gebäudes mit einer Animation aufgefächert und mit dem Slider durchlaufen.
Schließlich wird das ausgewählte Stockwerk angezeigt.
Alle Stockwerke oberhalb des Aktuellen werden ausgeblendet, die darunterliegenden werden durch das Aktuelle überdeckt.
\begin{figure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/wrdl_outdoor.png}
        \caption{}
        \label{sfig:wrld_outdoor}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/wrdl_indoor_g.png}
        \caption{}
        \label{sfig:wrld_indoor_g}
    \end{subfigure}
    \newline
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/wrdl_indoor_transition.png}
        \caption{}
        \label{sfig:wrld_indoor_transition}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/wrdl_indoor_2.png}
        \caption{}
        \label{sfig:wrld_indoor_2}
    \end{subfigure}
    \caption{Indoor-Funktionalität von WRLD.\@ %
        \subref{sfig:wrld_outdoor} Das Gebäude ist von außen sichtbar. %
        Über einen Button kann das Gebäude betreten werden. %
        \subref{sfig:wrld_indoor_g} Die Innenansicht des Gebäudes mitsamt Einrichtung wird angezeigt. %
        \subref{sfig:wrld_indoor_transition} Über einen Slider können die Stockwerke gewechselt werden. %
        \subref{sfig:wrld_indoor_2} Untenliegende Stockwerke werden vom aktuellen Stockwerk verdeckt.%
    }
    \label{fig:wrld_indoor}
\end{figure}

Für das Ziel dieser Arbeit (die Implementierung einer Indoor-Megamap im Stil von TCTD) wäre WRLD durch seine Indoor-Funktionalität eine ideale Grundlage.
Allerdings hat das SDK einen Nachteil, welches den Einsatz für diese Arbeit unmöglich macht.
Um eine Indoor-Karte in das SDK einzubinden, muss diese für das entsprechende Gebäude auf die WRDL-Server hochgeladen werden.
Im Fall dieser Arbeit wäre das eine Indoor-Karte für das MZH der Universität Bremen.
WRLDs Kartenabdeckung für Deutschland ist jedoch nur teilweise vorhanden; die Daten für Bremen fehlen komplett.
Das MZH steht somit beim Upload der Karte nicht zur Verfügung.
Die Recherche des Verfassers dieser Arbeit ergab keine Möglichkeit, den Upload auf die Server durch eine Offline-Nutzung des SDKs zu umgehen.
Daher muss bei der Implementierung der Megamap auf WRLD verzichtet werden.
Die Art der Darstellung und Integration der Indoor-Karte in die Umgebung sowie der Wechsel zwischen den Stockwerken dient jedoch als Inspiration für die Implementierung einer eigenen Lösung.

\subsection{Mapbox SDK}
% TODO: Grafik für nicht funktionierende Pipeline?
Als Alternative zu WRLD bietet sich das Mapbox SDK an.
Mapbox bietet Schnittstellen zur Anzeige von Karten, Routennavigation sowie Visualisierung von kartenbasierten Daten.
Ähnlich wie bei Google Maps werden die Karten in \emph{\enquote{Tiles}} geliefert.
Mapbox bietet sowohl Raster-Tiles (Satellitenbilder) als auch Vektor-Tiles (vektorbasierte Straßen- und Gebäudeumrisse) an.
Mapbox bezieht seine Daten über unterschiedliche Provider, einschließlich OpenStreetMap \autocite{Mapbox2018}.
Über ein Plugin lässt sich Mapbox als Paket in Unity einbinden.
Mit dem \unity{AbstractMap}-Prefab kann auf die Hauptfunktionalität von Mapbox zugegriffen werden.

Mapbox erlaubt wie WRLD eine 3D-Ansicht der Karte, inklusive Gebäude.
Sofern die OpenStreetMap-Daten Informationen über die Beschaffenheit der Gebäude liefern, wie z.B. die Höhe, können diese Daten für eine 3D-Visualisierung mit Mapbox ausgelesen werden.
Dazu wird als Grundlage ein Vektor-Tile der Umgebung verwendet.
Für die Umrisse der Gebäude werden dann Polygone generiert und entsprechend der Höhe extrudiert.
\autoref{fig:mapbox_unity_uni_bremen} zeigt eine 3D-Ansicht des zentralen Gebäudekomplexes der Universität Bremen.
Die hier verwendeten Gebäudedaten stammen alle von OpenStreetMap.
Über weitere Einstellungen ließen sich weiterhin die farbliche Darstellung der Gebäude anpassen.
Die Darstellung weist allerdings Probleme auf, auf die später näher eingegangen wird.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/mapbox_unity_uni_bremen_markings}
    \caption{3D-Darstellung der Universität Bremen via Mapbox und Unity. %
    Die Gebäudedaten bezieht Mapbox von OpenStreetMap. %
    \textcolor{red}{Rot:} Fehlende Geometrie beim MZH.\@ %
    \textcolor[HTML]{00bdff}{Blau:} Fehlende Geometrie beim GW2. %
    (\textit{Farbversion dieses Dokuments wird empfohlen.})}
    \label{fig:mapbox_unity_uni_bremen}
\end{figure}

Anders als bei WRLD gibt es bei Mapbox keine \emph{explizite} Unterstützung von Indoor-Karten.
Jedoch lassen sich diese über einen Umweg einbinden, wie \textcites{Mapbox2018b}{Pavani2018}{Clarke2017} präsentieren.
Zuerst muss ein georeferenzierter Lageplan des Gebäudes erstellt werden.
Dies kann z.B. mit der Software \emph{QGIS} umgesetzt werden.
Für diese Arbeit wurde zunächst die Ebene 5 des MZH an der Universität Bremen basierend auf dem Lageplan des Gebäudes in QGIS georeferenziert.
Beim Georeferenzieren werden die Räume und Wände durch Polygone dargestellt.
Jeder Vertex besitzt neben seiner x- und y-Position eine weitere Koordinate in Bezug auf ein geographisches Koordinaten-Referenz-System (KRS).
% TODO: Mapbox' KRS einfügen
Mapbox verwendet hier das \texttt{???} KRS, wodurch jeder Vertex einen Breitengrad (\emph{\enquote{Latitude}}) im Bereich \ang{-90} bis \ang{90} und einen Längengrad (\emph{\enquote{Longitude}}) im Bereich \ang{-180} bis \ang{180} erhält.
Der \emph{Breitengrad} wird von Norden nach Süden gemessen, wobei der Äquator den Breitengrad \ang{0} hat.
Grade nördlich vom Äquator sind daher positiv, Grade südlich vom Äquator sind negativ.
Der \emph{Längengrad} wird von Osten nach Westen gemessen.
Dabei hat der \emph{Nullmeridian} (\emph{\enquote{Prime Meridian}}; der Halbkreis vom Nordpol durch Greenwich in England bis zum Südpol) den Längengrad \ang{0}.
Grade östlich vom Nullmeridian sind positiv, Grade westlich vom Nullmeridian sind negativ \autocite{ESRIInc2018}.
Jeder Punkt auf der Erdoberfläche kann durch Angabe des Breiten- und Längengrads referenziert werden.
\autoref{fig:latlong_from_globe_center} zeigt dies graphisch.
Als Beispiel hat der Mittelpunkt des MZH in etwa den Breitengrad \ang{53,10668} und den Längengrad \ang{8,85238}.

\begin{figure}
    \centering
    \imagebox{\includegraphics[width=0.35\textwidth]{figures/latlong_from_globe_center}}
    \caption{(1) Breitengrad (2) Längengrad (3) \ang[detect-weight=true]{50} östlich (4) \ang[detect-weight=true]{40} nördlich. \quelle{\cite{ESRIInc2018}}}
    \label{fig:latlong_from_globe_center}
\end{figure}

Durch das Georeferenzieren können die Räume eindeutig auf einer Karte platziert werden.
Der georeferenzierte Lageplan der Ebene 5 des MZH ist in \autoref{fig:qgis_mzh_e5} zu sehen.
Die Räume werden durch die blauen Polygone dargestellt und die Wände durch die grauen.

\begin{figure}
    \centering
    \imagebox{\includegraphics[width=0.3\textwidth]{figures/qgis_mzh_e5}}
    \caption{Georeferenzierter Lageplan der Ebene 5 des MZH in QGIS.\@ %
    Die blauen Features sind vom Typ \lstinline{room}, die grauen vom Typ \lstinline{wall}.}
    \label{fig:qgis_mzh_e5}
\end{figure}

Theoretisch ließen sich die Wände anstatt durch Polygone auch durch Linien-Primitive darstellen.
Dies führt bei der 3D-Visualisierung von Mapbox zu Problemen bei der Darstellung von Ecken, wie \autoref{fig:mapbox_wand_ecke_problem} zeigt.
Mapbox generiert für die Linien-Wände Meshes mit einer gewissen Dicke.
Die generierten Meshes werden an Eckpunkten jedoch nicht automatisch verbunden.
Das heißt, dass weitere Ecken entstehen, was nicht der gewünschten Geometrie entspricht.
Das Modellieren der Wände durch Polygone erlaubt mehr Kontrolle über das Aussehen des generierten Meshes.

\begin{figure}
    \centering
    \imagebox{\includegraphics[width=0.6\textwidth]{figures/mapbox_wand_ecke_problem}}
    \vspace{2em}
    \caption{Wenn Wände durch Linien-Primitive repräsentiert werden, generiert Mapbox Meshes, die nicht automatisch verbunden werden.}
    \label{fig:mapbox_wand_ecke_problem}
\end{figure}

Durch das Hochladen auf die Mapbox-Server wird der Lageplan in ein Vektor-Tileset konvertiert.
Auf dieses kann mit dem Unity-Plugin zugegriffen werden.
\autoref{fig:mapbox_unity_mzh_e5_z15} zeigt die 3D-Darstellung des zuvor erstellten Lageplans.
Dabei werden nur Polygon-Features mit dem Attribut \lstinline{type = wall} gerendert (das Attribut wird in QGIS gesetzt).
Als Höhe wurde in diesem Beispiel der Wert 10 gewählt.
Wie zu erkennen ist, wurde die generierte Indoor-Karte an der korrekten Position auf der Stadtkarte platziert.
% TODO: Fenster u. Türen erwähnen?

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/mapbox_unity_mzh_e5_z15_working}
    \caption{3D-Darstellung des Lageplans der Ebene 5 im MZH.\@ %
    Nur die Wände werden hier angezeigt.}
    \label{fig:mapbox_unity_mzh_e5_z15}
\end{figure}

Im Verlauf einer ersten Implementierung einer Megamap basierend auf Mapbox ergaben sich Probleme, die auf die im Folgenden näher eingegangen wird.

\subsubsection*{Detailverlust und visuelle Artefakte bei unterschiedlichen Zoom-Ebenen}
Ein Nachteil beim zuvor genannten Ansatz ist, dass die erstellten Indoor-Polygone im Vergleich zu Gebäuden oder sogar ganzen Ländern sehr klein sind.
Die Breiten-/Längengrade der Vertices unterscheiden sich daher erst nach einigen Nachkommastellen.
Obwohl Mapbox intern die Koordinaten als 64-Bit-Werte repräsentiert \autocite{Kahyaoglu2017}, reicht bei einem niedrigen Zoom der Karte die Präzision zur Darstellung der Indoor-Objekte nicht aus.
\autoref{fig:mapbox_unity_mzh_e5_problems} zeigt das Problem auf mehreren Zoom-Stufen.
Bei einem niedrigen Zoom werden einige der Wände nicht gerendert, da vereinzelte Vertices nicht mehr unterschieden werden.
% TODO: Weiter ausführen... -> Github Issue

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/mapbox_unity_mzh_e5_z9}
        \caption{\lstinline{zoom = 9}}
        \label{sfig:mapbox_unity_mzh_e5_z9}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/mapbox_unity_mzh_e5_z11}
        \caption{\lstinline{zoom = 11}}
        \label{sfig:mapbox_unity_mzh_e5_z11}
    \end{subfigure}
    \newline
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/mapbox_unity_mzh_e5_z18}
        \caption{\lstinline{zoom = 18}}
        \label{sfig:mapbox_unity_mzh_e5_z18}
    \end{subfigure}
    \caption{Fehlende Wände und visuelle Artefakte bei unterschiedlichen Zoom-Ebenen.}
    \label{fig:mapbox_unity_mzh_e5_problems}
\end{figure}

Ein weiteres Problem ist, dass bei einem hohen Zoom visuelle Artefakte entstehen (siehe \autoref{sfig:mapbox_unity_mzh_e5_z18}).
Hier werden sowohl eine \enquote{Decke} als auch eine diagonale Wand eingezeichnet, die im originalen Datensatz nicht vorhanden sind.
Die Indoor-Karte befinden sich in diesem Fall auf der Schnittgrenze zweier Tiles.
Da Mapbox ein bekanntes Problem mit Gebäuden hat, die sich auf der Grenze mehrerer Tiles befinden, ist naheliegend, dass das Problem daher resultiert \autocite{Mapbox2018c}.

\subsubsection*{Ersetzung von OpenStreetMap-Gebäuden nicht möglich}
Wie bereits erwähnt ist in \autoref{fig:mapbox_unity_uni_bremen} der zentrale Gebäudekomplex der Universität Bremen abgebildet.
Die farblichen Markierungen zeigen jedoch, dass Teile mancher Gebäude (in diesem Fall das MZH und das GW2) fehlen.
Ein Gebäude kann in OSM durch die Zusammensetzung mehrerer Polygone definiert werden, um komplexere Strukturen wie z.B. unterschiedlich hohe Dachebenen darstellen zu können.
Die Technik wird auch für das MZH und das GW2 angewendet.
Es ist unklar, warum Teile des OSM-Datensatzes in Mapbox nicht angezeigt werden.
Zum Vergleich zeigt \autoref{fig:uni-bremen_osm2world}, wie die Gebäude eigentlich in der 3D-Darstellung aussehen sollten.
Für das Bild wurde direkt von OSM der entsprechende Ausschnitt der Uni exportiert und mit dem Programm \emph{OSM2World} gerendert.
Es wird deutlich, dass die Gebäudedaten in der Tat in der OSM-Datenbank vorhanden sind, weshalb das Fehlen dieser bei der Darstellung in Unity auf Mapbox zurückzuführen ist.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/uni-bremen_osm2world}
    \caption{Von OSM2World gerendertes Bild der Universität Bremen, basierend auf Daten aus OSM.\@ %
    Insbesondere das MZH und das GW2 werden korrekt dargestellt.%
    }
    \label{fig:uni-bremen_osm2world}
\end{figure}

Theoretisch wäre es möglich, das fehlerhafte Gebäudemodell durch ein korrigiertes zu ersetzen.
Mapbox bietet hierfür einen \lstinline{ReplaceFeatureModifier} an.
Diesem kann ein Breiten- und Längengrad sowie ein Unity-Prefab übergeben werden.
Falls sich an den angegebenen Koordinaten ein Feature befindet (z.B. ein Gebäude), wird dieses gelöscht und stattdessen wird das Prefab auf der Karte platziert \autocite{Mapbox2018d}.

Allerdings funktioniert dieser \emph{Modifier} nur, wenn die \unity{AbstractMap} so eingestellt ist, dass nur Gebäude mit einer eindeutigen ID angezeigt werden.
Durch das Aktivieren dieser Einstellung werden jedoch in einigen Regionen, darunter auch Bremen, gar keine Gebäude angezeigt.
Werden wiederum die eindeutigen IDs für Gebäude nicht aktiviert, wirft Mapbox intern einen Fehler.
Dieser ist darauf zurückzuführen, dass nun für die betreffenden Gebäude sehr kurze IDs generiert werden (von 0 aufwärts gezählt) \autocite{Github2018}.

Folglich besteht keine Möglichkeit, die fehlerhaft generierten Gebäude zu entfernen und diese durch die korrigierten Varianten zu ersetzen.
Da außerdem die Darstellung der Indoor-Karte stark von den Einstellungen der \unity{AbstractMap} abhängt und wenig Kontrolle über die Visualisierung von Details wie Türen, Fenster oder Einrichtungsgegenstände bietet, wird das Mapbox SDK für die Implementierung einer Indoor-Megamap ausgeschlossen.

%
\cleardoublepage
